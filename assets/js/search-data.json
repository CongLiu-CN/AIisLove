{
  
    
        "post0": {
            "title": "Title",
            "content": "Getting our data ready to be used with machine learning . # Standard imports import pandas as pd import numpy as np import matplotlib.pyplot as plt %matplotlib inline . Three main things we have to do: . 1. Splitting the data into features and labels (usually `X` &amp; `y`) 2. Filling (also called imputing) or disregarding missing values 3. Converting non-numerical values to numerical values (also called feature encoding) . Let&#39;s still use our heart disease data by importing it from Github: . heart_disease = pd.read_csv(&quot;https://raw.githubusercontent.com/CongLiu-CN/zero-to-mastery-ml/master/data/heart-disease.csv&quot;) heart_disease.head() . age sex cp trestbps chol fbs restecg thalach exang oldpeak slope ca thal target . 0 63 | 1 | 3 | 145 | 233 | 1 | 0 | 150 | 0 | 2.3 | 0 | 0 | 1 | 1 | . 1 37 | 1 | 2 | 130 | 250 | 0 | 1 | 187 | 0 | 3.5 | 0 | 0 | 2 | 1 | . 2 41 | 0 | 1 | 130 | 204 | 0 | 0 | 172 | 0 | 1.4 | 2 | 0 | 2 | 1 | . 3 56 | 1 | 1 | 120 | 236 | 0 | 1 | 178 | 0 | 0.8 | 2 | 0 | 2 | 1 | . 4 57 | 0 | 0 | 120 | 354 | 0 | 1 | 163 | 1 | 0.6 | 2 | 0 | 2 | 1 | . X = heart_disease.drop(&quot;target&quot;, axis=1) X.head() . age sex cp trestbps chol fbs restecg thalach exang oldpeak slope ca thal . 0 63 | 1 | 3 | 145 | 233 | 1 | 0 | 150 | 0 | 2.3 | 0 | 0 | 1 | . 1 37 | 1 | 2 | 130 | 250 | 0 | 1 | 187 | 0 | 3.5 | 0 | 0 | 2 | . 2 41 | 0 | 1 | 130 | 204 | 0 | 0 | 172 | 0 | 1.4 | 2 | 0 | 2 | . 3 56 | 1 | 1 | 120 | 236 | 0 | 1 | 178 | 0 | 0.8 | 2 | 0 | 2 | . 4 57 | 0 | 0 | 120 | 354 | 0 | 1 | 163 | 1 | 0.6 | 2 | 0 | 2 | . y = heart_disease[&quot;target&quot;] y.head() . 0 1 1 1 2 1 3 1 4 1 Name: target, dtype: int64 . 1. Split the data into training and test sets: . from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2) X_train.shape, X_test.shape, y_train.shape, y_test.shape . ((242, 13), (61, 13), (242,), (61,)) . X.shape[0] * 0.8, X_train.shape[0] . (242.4, 242) . 242 + 61 == len(heart_disease) . True . 2. Make sure it&#39;s all numerical . car_sales = pd.read_csv(&quot;https://raw.githubusercontent.com/CongLiu-CN/zero-to-mastery-ml/master/data/car-sales-extended.csv&quot;) car_sales.head() . Make Colour Odometer (KM) Doors Price . 0 Honda | White | 35431 | 4 | 15323 | . 1 BMW | Blue | 192714 | 5 | 19943 | . 2 Honda | White | 84714 | 4 | 28343 | . 3 Toyota | White | 154365 | 4 | 13434 | . 4 Nissan | Blue | 181577 | 3 | 14043 | . len(car_sales) . 1000 . car_sales.dtypes . Make object Colour object Odometer (KM) int64 Doors int64 Price int64 dtype: object . Split the data into X &amp; y: . X = car_sales.drop(&quot;Price&quot;, axis=1) y = car_sales[&quot;Price&quot;] . Split the data into training &amp; test: . X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2) . Build machine learning model: . from sklearn.ensemble import RandomForestRegressor model = RandomForestRegressor() model.fit(X_train, y_train) model.score(X_test, y_test) . - ValueError Traceback (most recent call last) &lt;ipython-input-13-4b70515c7cdf&gt; in &lt;module&gt; 2 3 model = RandomForestRegressor() -&gt; 4 model.fit(X_train, y_train) 5 model.score(X_test, y_test) E: machine_learning env lib site-packages sklearn ensemble _forest.py in fit(self, X, y, sample_weight) 293 &#34;&#34;&#34; 294 # Validate or convert input data --&gt; 295 X = check_array(X, accept_sparse=&#34;csc&#34;, dtype=DTYPE) 296 y = check_array(y, accept_sparse=&#39;csc&#39;, ensure_2d=False, dtype=None) 297 if sample_weight is not None: E: machine_learning env lib site-packages sklearn utils validation.py in check_array(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator) 529 array = array.astype(dtype, casting=&#34;unsafe&#34;, copy=False) 530 else: --&gt; 531 array = np.asarray(array, order=order, dtype=dtype) 532 except ComplexWarning: 533 raise ValueError(&#34;Complex data not supported n&#34; E: machine_learning env lib site-packages numpy core _asarray.py in asarray(a, dtype, order) 83 84 &#34;&#34;&#34; &gt; 85 return array(a, dtype, copy=False, order=order) 86 87 ValueError: could not convert string to float: &#39;BMW&#39; . Machine learning model can&#39;t handle strings, so we need to convert those strings into numbers. . Turn the categories into numbers: . from sklearn.preprocessing import OneHotEncoder from sklearn.compose import ColumnTransformer categorical_features = [&quot;Make&quot;, &quot;Colour&quot;, &quot;Doors&quot;] one_hot = OneHotEncoder() transformer = ColumnTransformer([(&quot;one_hot&quot;, one_hot, categorical_features)], remainder=&quot;passthrough&quot;) transformed_X = transformer.fit_transform(X) transformed_X . array([[0.00000e+00, 1.00000e+00, 0.00000e+00, ..., 1.00000e+00, 0.00000e+00, 3.54310e+04], [1.00000e+00, 0.00000e+00, 0.00000e+00, ..., 0.00000e+00, 1.00000e+00, 1.92714e+05], [0.00000e+00, 1.00000e+00, 0.00000e+00, ..., 1.00000e+00, 0.00000e+00, 8.47140e+04], ..., [0.00000e+00, 0.00000e+00, 1.00000e+00, ..., 1.00000e+00, 0.00000e+00, 6.66040e+04], [0.00000e+00, 1.00000e+00, 0.00000e+00, ..., 1.00000e+00, 0.00000e+00, 2.15883e+05], [0.00000e+00, 0.00000e+00, 0.00000e+00, ..., 1.00000e+00, 0.00000e+00, 2.48360e+05]]) . pd.DataFrame(transformed_X) . 0 1 2 3 4 5 6 7 8 9 10 11 12 . 0 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 35431.0 | . 1 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 192714.0 | . 2 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 84714.0 | . 3 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 154365.0 | . 4 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 181577.0 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 995 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 35820.0 | . 996 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 0.0 | 0.0 | 155144.0 | . 997 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 66604.0 | . 998 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 215883.0 | . 999 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 248360.0 | . 1000 rows × 13 columns . What OneHotEncoder does under the hood: . . dummies = pd.get_dummies(car_sales[[&quot;Make&quot;, &quot;Colour&quot;, &quot;Doors&quot;]]) dummies . Doors Make_BMW Make_Honda Make_Nissan Make_Toyota Colour_Black Colour_Blue Colour_Green Colour_Red Colour_White . 0 4 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | . 1 5 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | . 2 4 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | . 3 4 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | . 4 3 | 0 | 0 | 1 | 0 | 0 | 1 | 0 | 0 | 0 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 995 4 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | . 996 3 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 1 | . 997 4 | 0 | 0 | 1 | 0 | 0 | 1 | 0 | 0 | 0 | . 998 4 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | . 999 4 | 0 | 0 | 0 | 1 | 0 | 1 | 0 | 0 | 0 | . 1000 rows × 10 columns . Let&#39;s refit the model: . np.random.seed(2020) X_train, X_test, y_train, y_test = train_test_split(transformed_X, y, test_size=0.2) model.fit(X_train, y_train) . RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion=&#39;mse&#39;, max_depth=None, max_features=&#39;auto&#39;, max_leaf_nodes=None, max_samples=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None, oob_score=False, random_state=None, verbose=0, warm_start=False) . model.score(X_test, y_test) . 0.19383319494749351 . Why the score is so low? . --Intentionally blank line-- . 3. What if there were missing values? . Fill them with some value (also known as imputation). | Remove the samples with missing data altogether. | This time what we import is the car sales data with missing values: . car_sales_missing = pd.read_csv(&quot;https://raw.githubusercontent.com/CongLiu-CN/zero-to-mastery-ml/master/data/car-sales-extended-missing-data.csv&quot;) car_sales_missing.head() . Make Colour Odometer (KM) Doors Price . 0 Honda | White | 35431.0 | 4.0 | 15323.0 | . 1 BMW | Blue | 192714.0 | 5.0 | 19943.0 | . 2 Honda | White | 84714.0 | 4.0 | 28343.0 | . 3 Toyota | White | 154365.0 | 4.0 | 13434.0 | . 4 Nissan | Blue | 181577.0 | 3.0 | 14043.0 | . car_sales_missing.isna().sum() . Make 49 Colour 50 Odometer (KM) 50 Doors 50 Price 50 dtype: int64 . Create X &amp; y: . X = car_sales_missing.drop(&quot;Price&quot;, axis=1) y = car_sales_missing[&quot;Price&quot;] . Let&#39;s try and convert our data to numbers. . Turn the categories into numbers: . from sklearn.preprocessing import OneHotEncoder from sklearn.compose import ColumnTransformer categorical_features = [&quot;Make&quot;, &quot;Colour&quot;, &quot;Doors&quot;] one_hot = OneHotEncoder() transformer = ColumnTransformer([(&quot;one_hot&quot;, one_hot, categorical_features)], remainder=&quot;passthrough&quot;) transformed_X = transformer.fit_transform(X) transformed_X . - ValueError Traceback (most recent call last) &lt;ipython-input-22-c22fbfad0141&gt; in &lt;module&gt; 9 remainder=&#34;passthrough&#34;) 10 &gt; 11 transformed_X = transformer.fit_transform(X) 12 transformed_X E: machine_learning env lib site-packages sklearn compose _column_transformer.py in fit_transform(self, X, y) 516 self._validate_remainder(X) 517 --&gt; 518 result = self._fit_transform(X, y, _fit_transform_one) 519 520 if not result: E: machine_learning env lib site-packages sklearn compose _column_transformer.py in _fit_transform(self, X, y, func, fitted) 446 self._iter(fitted=fitted, replace_strings=True)) 447 try: --&gt; 448 return Parallel(n_jobs=self.n_jobs)( 449 delayed(func)( 450 transformer=clone(trans) if not fitted else trans, E: machine_learning env lib site-packages joblib parallel.py in __call__(self, iterable) 1002 # remaining jobs. 1003 self._iterating = False -&gt; 1004 if self.dispatch_one_batch(iterator): 1005 self._iterating = self._original_iterator is not None 1006 E: machine_learning env lib site-packages joblib parallel.py in dispatch_one_batch(self, iterator) 833 return False 834 else: --&gt; 835 self._dispatch(tasks) 836 return True 837 E: machine_learning env lib site-packages joblib parallel.py in _dispatch(self, batch) 752 with self._lock: 753 job_idx = len(self._jobs) --&gt; 754 job = self._backend.apply_async(batch, callback=cb) 755 # A job can complete so quickly than its callback is 756 # called before we get here, causing self._jobs to E: machine_learning env lib site-packages joblib _parallel_backends.py in apply_async(self, func, callback) 207 def apply_async(self, func, callback=None): 208 &#34;&#34;&#34;Schedule a func to be run&#34;&#34;&#34; --&gt; 209 result = ImmediateResult(func) 210 if callback: 211 callback(result) E: machine_learning env lib site-packages joblib _parallel_backends.py in __init__(self, batch) 588 # Don&#39;t delay the application, to avoid keeping the input 589 # arguments in memory --&gt; 590 self.results = batch() 591 592 def get(self): E: machine_learning env lib site-packages joblib parallel.py in __call__(self) 253 # change the default number of processes to -1 254 with parallel_backend(self._backend, n_jobs=self._n_jobs): --&gt; 255 return [func(*args, **kwargs) 256 for func, args, kwargs in self.items] 257 E: machine_learning env lib site-packages joblib parallel.py in &lt;listcomp&gt;(.0) 253 # change the default number of processes to -1 254 with parallel_backend(self._backend, n_jobs=self._n_jobs): --&gt; 255 return [func(*args, **kwargs) 256 for func, args, kwargs in self.items] 257 E: machine_learning env lib site-packages sklearn pipeline.py in _fit_transform_one(transformer, X, y, weight, message_clsname, message, **fit_params) 726 with _print_elapsed_time(message_clsname, message): 727 if hasattr(transformer, &#39;fit_transform&#39;): --&gt; 728 res = transformer.fit_transform(X, y, **fit_params) 729 else: 730 res = transformer.fit(X, y, **fit_params).transform(X) E: machine_learning env lib site-packages sklearn preprocessing _encoders.py in fit_transform(self, X, y) 370 &#34;&#34;&#34; 371 self._validate_keywords() --&gt; 372 return super().fit_transform(X, y) 373 374 def transform(self, X): E: machine_learning env lib site-packages sklearn base.py in fit_transform(self, X, y, **fit_params) 569 if y is None: 570 # fit method of arity 1 (unsupervised transformation) --&gt; 571 return self.fit(X, **fit_params).transform(X) 572 else: 573 # fit method of arity 2 (supervised transformation) E: machine_learning env lib site-packages sklearn preprocessing _encoders.py in fit(self, X, y) 345 &#34;&#34;&#34; 346 self._validate_keywords() --&gt; 347 self._fit(X, handle_unknown=self.handle_unknown) 348 self.drop_idx_ = self._compute_drop_idx() 349 return self E: machine_learning env lib site-packages sklearn preprocessing _encoders.py in _fit(self, X, handle_unknown) 72 73 def _fit(self, X, handle_unknown=&#39;error&#39;): &gt; 74 X_list, n_samples, n_features = self._check_X(X) 75 76 if self.categories != &#39;auto&#39;: E: machine_learning env lib site-packages sklearn preprocessing _encoders.py in _check_X(self, X) 58 for i in range(n_features): 59 Xi = self._get_feature(X, feature_idx=i) &gt; 60 Xi = check_array(Xi, ensure_2d=False, dtype=None, 61 force_all_finite=needs_validation) 62 X_columns.append(Xi) E: machine_learning env lib site-packages sklearn utils validation.py in check_array(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator) 575 576 if force_all_finite: --&gt; 577 _assert_all_finite(array, 578 allow_nan=force_all_finite == &#39;allow-nan&#39;) 579 E: machine_learning env lib site-packages sklearn utils validation.py in _assert_all_finite(X, allow_nan, msg_dtype) 63 elif X.dtype == np.dtype(&#39;object&#39;) and not allow_nan: 64 if _object_dtype_isnan(X).any(): &gt; 65 raise ValueError(&#34;Input contains NaN&#34;) 66 67 ValueError: Input contains NaN . Why there is an error? . --Intentionally blank line-- . car_sales_missing[&quot;Doors&quot;].value_counts() . 4.0 811 5.0 75 3.0 64 Name: Doors, dtype: int64 . Option 1: Fill missing data with Pandas . Learn more about filling missing values with pandas: https://www.geeksforgeeks.org/python-pandas-dataframe-fillna-to-replace-null-values-in-dataframe/ . # Fill the &quot;Make&quot; column car_sales_missing[&quot;Make&quot;].fillna(&quot;missing&quot;, inplace=True) # Fill the &quot;Colour&quot; column car_sales_missing[&quot;Colour&quot;].fillna(&quot;missing&quot;, inplace=True) # Fill the &quot;Odometer (KM)&quot; column car_sales_missing[&quot;Odometer (KM)&quot;].fillna(car_sales_missing[&quot;Odometer (KM)&quot;].mean(), inplace=True) # Fill the &quot;Doors&quot; column car_sales_missing[&quot;Doors&quot;].fillna(4, inplace=True) . # Check our dataframe again car_sales_missing.isna().sum() . Make 0 Colour 0 Odometer (KM) 0 Doors 0 Price 50 dtype: int64 . # Remove rows with missing Price value car_sales_missing.dropna(inplace=True) . car_sales_missing.isna().sum() . Make 0 Colour 0 Odometer (KM) 0 Doors 0 Price 0 dtype: int64 . Now we have lost 50 data because of the removed data in &quot;Price&quot; column. . len(car_sales_missing) . 950 . The missing values are all filled or removed, so we can convert them. . X = car_sales_missing.drop(&quot;Price&quot;, axis=1) y = car_sales_missing[&quot;Price&quot;] . Let&#39;s try and convert our data to numbers. Turn the categories into numbers: . from sklearn.preprocessing import OneHotEncoder from sklearn.compose import ColumnTransformer categorical_features = [&quot;Make&quot;, &quot;Colour&quot;, &quot;Doors&quot;] one_hot = OneHotEncoder() transformer = ColumnTransformer([(&quot;one_hot&quot;, one_hot, categorical_features)], remainder=&quot;passthrough&quot;) transformed_X = transformer.fit_transform(car_sales_missing) transformed_X . array([[0.00000e+00, 1.00000e+00, 0.00000e+00, ..., 0.00000e+00, 3.54310e+04, 1.53230e+04], [1.00000e+00, 0.00000e+00, 0.00000e+00, ..., 1.00000e+00, 1.92714e+05, 1.99430e+04], [0.00000e+00, 1.00000e+00, 0.00000e+00, ..., 0.00000e+00, 8.47140e+04, 2.83430e+04], ..., [0.00000e+00, 0.00000e+00, 1.00000e+00, ..., 0.00000e+00, 6.66040e+04, 3.15700e+04], [0.00000e+00, 1.00000e+00, 0.00000e+00, ..., 0.00000e+00, 2.15883e+05, 4.00100e+03], [0.00000e+00, 0.00000e+00, 0.00000e+00, ..., 0.00000e+00, 2.48360e+05, 1.27320e+04]]) . Option 2: Filling missing data and transforming categorical data with Scikit-Learn . The main takeaways: . Split your data first, always keep your training &amp; test data separate | Fill/transform the training set and test sets separately | . # standard imports import numpy as np import pandas as pd import matplotlib.pyplot as plt %matplotlib inline . car_sales_missing = pd.read_csv(&quot;https://raw.githubusercontent.com/CongLiu-CN/zero-to-mastery-ml/master/data/car-sales-extended-missing-data.csv&quot;) car_sales_missing.head() . Make Colour Odometer (KM) Doors Price . 0 Honda | White | 35431.0 | 4.0 | 15323.0 | . 1 BMW | Blue | 192714.0 | 5.0 | 19943.0 | . 2 Honda | White | 84714.0 | 4.0 | 28343.0 | . 3 Toyota | White | 154365.0 | 4.0 | 13434.0 | . 4 Nissan | Blue | 181577.0 | 3.0 | 14043.0 | . # Check missing values car_sales_missing.isna().sum() . Make 49 Colour 50 Odometer (KM) 50 Doors 50 Price 50 dtype: int64 . # Drop the rows with no labels car_sales_missing.dropna(subset=[&quot;Price&quot;], inplace=True) car_sales_missing.isna().sum() . Make 47 Colour 46 Odometer (KM) 48 Doors 47 Price 0 dtype: int64 . Note the data is split before any filling or transformations. . from sklearn.model_selection import train_test_split # Split into X &amp; y X = car_sales_missing.drop(&quot;Price&quot;, axis=1) y = car_sales_missing[&quot;Price&quot;] # Split data into train and test np.random.seed(2020) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2) . # Check missing values X.isna().sum() . Make 47 Colour 46 Odometer (KM) 48 Doors 47 dtype: int64 . Let&#39;s fill the missing values. . We must fill the training and test values separately to ensure training data stays with the training dta and test data stays with the test data. . Note: We use fit_transform() on the training data and transform() on the testing data. In essence, we learn the patterns in the training set and transform it via imputation (fit, then transform). Then we take those same patterns and fill the test set (transform only). . # Fill missing values with Scikit-Learn from sklearn.impute import SimpleImputer from sklearn.compose import ColumnTransformer # Fill categorical values with &#39;missing&#39; &amp; numerical values with mean cat_imputer = SimpleImputer(strategy=&quot;constant&quot;, fill_value=&quot;missing&quot;) door_imputer = SimpleImputer(strategy=&quot;constant&quot;, fill_value=4) num_imputer = SimpleImputer(strategy=&quot;mean&quot;) # Define columns cat_features = [&quot;Make&quot;, &quot;Colour&quot;] door_feature = [&quot;Doors&quot;] num_features = [&quot;Odometer (KM)&quot;] # Create an imputer (something that fills missing data) imputer = ColumnTransformer([ (&quot;cat_imputer&quot;, cat_imputer, cat_features), (&quot;door_imputer&quot;, door_imputer, door_feature), (&quot;num_imputer&quot;, num_imputer, num_features) ]) # Fill train and test values separately filled_X_train = imputer.fit_transform(X_train) filled_X_test = imputer.transform(X_test) # Check filled X_train filled_X_train . array([[&#39;Nissan&#39;, &#39;Blue&#39;, 3.0, 24219.0], [&#39;Honda&#39;, &#39;Blue&#39;, 4.0, 219217.0], [&#39;Nissan&#39;, &#39;White&#39;, 4.0, 152215.0], ..., [&#39;Toyota&#39;, &#39;White&#39;, 4.0, 112292.0], [&#39;Toyota&#39;, &#39;White&#39;, 4.0, 83594.0], [&#39;Nissan&#39;, &#39;White&#39;, 3.0, 131296.40416666667]], dtype=object) . Now we&#39;ve filled our missing values, let&#39;s check how many are missing from each set. (Should be 0) . # Get our transformed data arrays back into DataFrames car_sales_filled_train = pd.DataFrame(filled_X_train, columns=[&quot;Make&quot;, &quot;Colour&quot;, &quot;Doors&quot;, &quot;Odometer (KM)&quot;]) car_sales_filled_test = pd.DataFrame(filled_X_test, columns=[&quot;Make&quot;, &quot;Colour&quot;, &quot;Doors&quot;, &quot;Odometer (KM)&quot;]) # Check missing data in training set car_sales_filled_train.isna().sum() . Make 0 Colour 0 Doors 0 Odometer (KM) 0 dtype: int64 . # Check missing data in test set car_sales_filled_test.isna().sum() . Make 0 Colour 0 Doors 0 Odometer (KM) 0 dtype: int64 . Look! No more missing values! . # Check to see the original... still missing values car_sales_missing.isna().sum() . Make 47 Colour 46 Odometer (KM) 48 Doors 47 Price 0 dtype: int64 . Ok, no more missing values but we&#39;ve still got to turn our data into numbers. . Let&#39;s do that using one hot encoding. . Again, keeping our training and test data separate. . # Import OneHotEncoder class from sklearn from sklearn.preprocessing import OneHotEncoder # Now let&#39;s one hot encode the features with the same code as before categorical_features = [&quot;Make&quot;, &quot;Colour&quot;, &quot;Doors&quot;] one_hot = OneHotEncoder() transformer = ColumnTransformer([(&quot;one_hot&quot;, one_hot, categorical_features)], remainder=&quot;passthrough&quot;) # Fill train and test values separately transformed_X_train = transformer.fit_transform(car_sales_filled_train) transformed_X_test = transformer.transform(car_sales_filled_test) # Check transformed and filled X_train transformed_X_train.toarray() . array([[0.00000000e+00, 0.00000000e+00, 1.00000000e+00, ..., 0.00000000e+00, 0.00000000e+00, 2.42190000e+04], [0.00000000e+00, 1.00000000e+00, 0.00000000e+00, ..., 1.00000000e+00, 0.00000000e+00, 2.19217000e+05], [0.00000000e+00, 0.00000000e+00, 1.00000000e+00, ..., 1.00000000e+00, 0.00000000e+00, 1.52215000e+05], ..., [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ..., 1.00000000e+00, 0.00000000e+00, 1.12292000e+05], [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ..., 1.00000000e+00, 0.00000000e+00, 8.35940000e+04], [0.00000000e+00, 0.00000000e+00, 1.00000000e+00, ..., 0.00000000e+00, 0.00000000e+00, 1.31296404e+05]]) . Fit a model . Wonderful! Now we&#39;ve filled and transformed our data, ensuring the training and test sets have been kept separate. Let&#39;s fit a model to the training set and evaluate it on the test set. . # Since we&#39;ve transformed X, let&#39;s see if we can fit a model np.random.seed(2020) from sklearn.ensemble import RandomForestRegressor # Setup model model = RandomForestRegressor() # Make sure to use transformed (filled and one-hot encoded X data) model.fit(transformed_X_train, y_train) model.score(transformed_X_test, y_test) . 0.07839674412816233 . Why the score is even lower than the previous one? . --Becaue of the data we lost-- . len(car_sales), len(car_sales_missing) . (1000, 950) . The process of filling missing values is called Imputation. . The process of turn non-numerical values into numerical values is called Feature Engineering or Fearure Encoding. .",
            "url": "https://www.ai4love.xyz//2020/04/04/Splitting-Your-Data.html",
            "relUrl": "/2020/04/04/Splitting-Your-Data.html",
            "date": " • Apr 4, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Introduction to Scikit-Learn",
            "content": "This notebook records the 8.1 lesson of Daniel&#39;s ML course. He provides a summary of the chapter 8, which is listed below: . An end-to-end Scikit-Learn workflow | Getting the data ready | Choose the right estimator/algorithm for our problems | Fit the model/algorithm and use it to make predictions on our data | Evaluating a model | Improve a model | Save and load a trained model | Putting it all together | In this lesson, we focus only on the first section—An end-to-end Scikit-Learn workflow. This section is a brief overview of the entire chapter 8. Therefore, the content of this section can be a bit overwelming. There is no need to fully understand every points in this part. Its main purpose is only give students an opportunity to preview what they are going to learn later to better understand and use sklearn. . Daniel, the instructor of this course, shows every step of a common sklearn workflow in a simple version. Those steps are &quot;1. GET THE DATA READY&quot;, &quot;2. CHOOSE THE RIGHT MODEL AND HYPERPARAMETERS&quot;, &quot;3. FIT THE MODEL TO THE TRAINING DATA&quot;, &quot;4. EVALUATE THE MODEL ON THE TRAINING DATA AND TEST DATA&quot;, &quot;5. IMPROVE A MODEL&quot;, &quot;6. SAVE A MODEL AND LOAD IT&quot;. The last step in the list above is missing for unknown reasons. . 1. GET THE DATA READY . The first thing we need to do is not importing the data, but rather, importing some common libraries, such as pandas, numpy, matplotlib, and sklearn. But for now, let&#39;s just import the first two. . import pandas as pd import numpy as np . You may ask what the hell is pandas or numpy? . Let me explain... . &quot;Pandas&quot; is a software library writtten for the Python programming language for data manipulation and analysis. More specificly, it offers data structrues and operations for manipulating numerical tables and time series. . &quot;Numpy is a library for the Python programming language, adding support for large, multi-dimenstional arrays and matrices, along with a large collection of high-level mathematical functions to operate on these arrays. . Hooo, that&#39;s a lot to take in, but we may be ok for now. . After we import pandas and numpy, let&#39;s import the heart disease data from Github: . heart_disease = pd.read_csv(&quot;https://raw.githubusercontent.com/CongLiu-CN/zero-to-mastery-ml/master/data/heart-disease.csv&quot;) heart_disease.head() . age sex cp trestbps chol fbs restecg thalach exang oldpeak slope ca thal target . 0 63 | 1 | 3 | 145 | 233 | 1 | 0 | 150 | 0 | 2.3 | 0 | 0 | 1 | 1 | . 1 37 | 1 | 2 | 130 | 250 | 0 | 1 | 187 | 0 | 3.5 | 0 | 0 | 2 | 1 | . 2 41 | 0 | 1 | 130 | 204 | 0 | 0 | 172 | 0 | 1.4 | 2 | 0 | 2 | 1 | . 3 56 | 1 | 1 | 120 | 236 | 0 | 1 | 178 | 0 | 0.8 | 2 | 0 | 2 | 1 | . 4 57 | 0 | 0 | 120 | 354 | 0 | 1 | 163 | 1 | 0.6 | 2 | 0 | 2 | 1 | . Why do we need to split data into X &amp; y? . --Intentionally blank line-- . X = heart_disease.drop(&quot;target&quot;, axis=1) y = heart_disease[&quot;target&quot;] . 2. Choose the right model and hyperparameters . from sklearn.ensemble import RandomForestClassifier clf = RandomForestClassifier() . # We&#39;ll keep the deafault hyperparameters clf.get_params() . {&#39;bootstrap&#39;: True, &#39;ccp_alpha&#39;: 0.0, &#39;class_weight&#39;: None, &#39;criterion&#39;: &#39;gini&#39;, &#39;max_depth&#39;: None, &#39;max_features&#39;: &#39;auto&#39;, &#39;max_leaf_nodes&#39;: None, &#39;max_samples&#39;: None, &#39;min_impurity_decrease&#39;: 0.0, &#39;min_impurity_split&#39;: None, &#39;min_samples_leaf&#39;: 1, &#39;min_samples_split&#39;: 2, &#39;min_weight_fraction_leaf&#39;: 0.0, &#39;n_estimators&#39;: 100, &#39;n_jobs&#39;: None, &#39;oob_score&#39;: False, &#39;random_state&#39;: None, &#39;verbose&#39;: 0, &#39;warm_start&#39;: False} . 3. Fit the model to the training data . from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2) . clf.fit(X_train, y_train); . Make a prediction: . y_preds = clf.predict(X_test) y_preds . array([0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0], dtype=int64) . y_test . 256 0 24 1 266 0 294 0 131 1 .. 14 1 146 1 275 0 25 1 168 0 Name: target, Length: 61, dtype: int64 . 4. Evaluate the model on the training data and test data . clf.score(X_train, y_train) . 1.0 . clf.score(X_test, y_test) . 0.9016393442622951 . from sklearn.metrics import classification_report, confusion_matrix, accuracy_score . print(classification_report(y_test, y_preds)) . precision recall f1-score support 0 0.92 0.86 0.89 28 1 0.89 0.94 0.91 33 accuracy 0.90 61 macro avg 0.90 0.90 0.90 61 weighted avg 0.90 0.90 0.90 61 . confusion_matrix(y_test, y_preds) . array([[24, 4], [ 2, 31]], dtype=int64) . accuracy_score(y_test, y_preds) . 0.9016393442622951 . 5. Improve a model . Try different amount of n_estimators . np.random.seed(2020) . for i in range(10, 100, 10): print(f&quot;Trying model with {i} estimators...&quot;) clf = RandomForestClassifier(n_estimators=i).fit(X_train, y_train) print(f&quot;Model accuracy on test set: {clf.score(X_test, y_test)*100:.2f}%&quot;) print(&quot;&quot;) . Trying model with 10 estimators... Model accuracy on test set: 78.69% Trying model with 20 estimators... Model accuracy on test set: 90.16% Trying model with 30 estimators... Model accuracy on test set: 85.25% Trying model with 40 estimators... Model accuracy on test set: 88.52% Trying model with 50 estimators... Model accuracy on test set: 90.16% Trying model with 60 estimators... Model accuracy on test set: 90.16% Trying model with 70 estimators... Model accuracy on test set: 86.89% Trying model with 80 estimators... Model accuracy on test set: 85.25% Trying model with 90 estimators... Model accuracy on test set: 91.80% . 6. Save a model and load it . What is pickle? . https://www.pitt.edu/~naraehan/python3/pickling.html . import pickle . pickle.dump(clf, open(&quot;random_forest_model_1.pkl&quot;, &quot;wb&quot;)) . loaded_model = pickle.load(open(&quot;random_forest_model_1.pkl&quot;, &quot;rb&quot;)) loaded_model.score(X_test, y_test) # align with the last result . 0.9180327868852459 .",
            "url": "https://www.ai4love.xyz//2020/04/03/Introduction-to-Scikit-Learn.html",
            "relUrl": "/2020/04/03/Introduction-to-Scikit-Learn.html",
            "date": " • Apr 3, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # Title &gt; Awesome summary - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(movies).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(movies).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=alt.Y(&#39;IMDB_Rating:Q&#39;, axis=alt.Axis(minExtent=30)), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=600, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=700, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; df = pd.read_json(movies) # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://www.ai4love.xyz//jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://www.ai4love.xyz//markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This is where you put the contents of your About page. Like all your pages, it’s in Markdown format. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://www.ai4love.xyz//about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

}