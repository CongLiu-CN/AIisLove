{
  
    
        "post0": {
            "title": "",
            "content": "Estimator score method | The scoring parameter | Problem-specific metric functions | 1. Evaluating a model with the score method . # Standard imports import pandas as pd import numpy as np import matplotlib.pyplot as plt %matplotlib inline . heart_disease = pd.read_csv(&quot;https://raw.githubusercontent.com/CongLiu-CN/zero-to-mastery-ml/master/data/heart-disease.csv&quot;) heart_disease.head() . age sex cp trestbps chol fbs restecg thalach exang oldpeak slope ca thal target . 0 63 | 1 | 3 | 145 | 233 | 1 | 0 | 150 | 0 | 2.3 | 0 | 0 | 1 | 1 | . 1 37 | 1 | 2 | 130 | 250 | 0 | 1 | 187 | 0 | 3.5 | 0 | 0 | 2 | 1 | . 2 41 | 0 | 1 | 130 | 204 | 0 | 0 | 172 | 0 | 1.4 | 2 | 0 | 2 | 1 | . 3 56 | 1 | 1 | 120 | 236 | 0 | 1 | 178 | 0 | 0.8 | 2 | 0 | 2 | 1 | . 4 57 | 0 | 0 | 120 | 354 | 0 | 1 | 163 | 1 | 0.6 | 2 | 0 | 2 | 1 | . from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import train_test_split np.random.seed(2020) X = heart_disease.drop(&quot;target&quot;, axis=1) y = heart_disease[&quot;target&quot;] X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2) clf = RandomForestClassifier() clf.fit(X_train, y_train) . RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None, criterion=&#39;gini&#39;, max_depth=None, max_features=&#39;auto&#39;, max_leaf_nodes=None, max_samples=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None, oob_score=False, random_state=None, verbose=0, warm_start=False) . clf.score(X_train, y_train) . 1.0 . clf.score(X_test, y_test) . 0.7868852459016393 . Let&#39;s do the same but for regression... . # Import Boston housing dataset from sklearn.datasets import load_boston boston = load_boston() boston; . # Turn boston into a pandas dataframe boston_df = pd.DataFrame(boston[&quot;data&quot;], columns=boston[&quot;feature_names&quot;]) boston_df[&quot;target&quot;] = pd.Series(boston[&quot;target&quot;]) boston_df.head() . CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX PTRATIO B LSTAT target . 0 0.00632 | 18.0 | 2.31 | 0.0 | 0.538 | 6.575 | 65.2 | 4.0900 | 1.0 | 296.0 | 15.3 | 396.90 | 4.98 | 24.0 | . 1 0.02731 | 0.0 | 7.07 | 0.0 | 0.469 | 6.421 | 78.9 | 4.9671 | 2.0 | 242.0 | 17.8 | 396.90 | 9.14 | 21.6 | . 2 0.02729 | 0.0 | 7.07 | 0.0 | 0.469 | 7.185 | 61.1 | 4.9671 | 2.0 | 242.0 | 17.8 | 392.83 | 4.03 | 34.7 | . 3 0.03237 | 0.0 | 2.18 | 0.0 | 0.458 | 6.998 | 45.8 | 6.0622 | 3.0 | 222.0 | 18.7 | 394.63 | 2.94 | 33.4 | . 4 0.06905 | 0.0 | 2.18 | 0.0 | 0.458 | 7.147 | 54.2 | 6.0622 | 3.0 | 222.0 | 18.7 | 396.90 | 5.33 | 36.2 | . from sklearn.ensemble import RandomForestRegressor np.random.seed(2020) # Create the data X = boston_df.drop(&quot;target&quot;, axis=1) y = boston_df[&quot;target&quot;] # Split into training and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2) # Instantiate and fit model model = RandomForestRegressor().fit(X_train, y_train) . model.score(X_test, y_test) . 0.7873300907872445 . 2. Evaluating a model using the scoring parameter . from sklearn.model_selection import cross_val_score from sklearn.ensemble import RandomForestClassifier np.random.seed(2020) X = heart_disease.drop(&quot;target&quot;, axis=1) y = heart_disease[&quot;target&quot;] X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2) clf = RandomForestClassifier() clf.fit(X_train, y_train); . clf.score(X_test, y_test) . 0.7868852459016393 . cross_val_score(clf, X, y) . array([0.81967213, 0.86885246, 0.78688525, 0.8 , 0.75 ]) . What does Cross-validation do under the hood? . . The sv=5 by default, but we can change it, for exapmle, to 10: . cross_val_score(clf, X, y, cv=10) . array([0.87096774, 0.74193548, 0.87096774, 0.9 , 0.9 , 0.8 , 0.73333333, 0.86666667, 0.76666667, 0.76666667]) . np.random.seed(2020) # Single training and test split score clf_single_score = clf.score(X_test, y_test) # Take the mean of 5-fold cross-validation score clf_cross_val_score = np.mean(cross_val_score(clf, X, y)) # Compare the two clf_single_score, clf_cross_val_score . (0.7868852459016393, 0.8149726775956283) . Default scoring parameter of classifier = mean accuracy . clf.score() . # Scoring parameter set to None by default cross_val_score(clf, X, y, cv=5, scoring=None) . array([0.83606557, 0.8852459 , 0.7704918 , 0.76666667, 0.78333333]) . So Why do we use cross-validation score? . To avoid getting lucky scores. . 3. Classification model evaluation metrics . Accuracy | Area under ROC curve | Confusion matrix | Classification report | 3.1 Accuracy . heart_disease.head() . age sex cp trestbps chol fbs restecg thalach exang oldpeak slope ca thal target . 0 63 | 1 | 3 | 145 | 233 | 1 | 0 | 150 | 0 | 2.3 | 0 | 0 | 1 | 1 | . 1 37 | 1 | 2 | 130 | 250 | 0 | 1 | 187 | 0 | 3.5 | 0 | 0 | 2 | 1 | . 2 41 | 0 | 1 | 130 | 204 | 0 | 0 | 172 | 0 | 1.4 | 2 | 0 | 2 | 1 | . 3 56 | 1 | 1 | 120 | 236 | 0 | 1 | 178 | 0 | 0.8 | 2 | 0 | 2 | 1 | . 4 57 | 0 | 0 | 120 | 354 | 0 | 1 | 163 | 1 | 0.6 | 2 | 0 | 2 | 1 | . from sklearn.model_selection import cross_val_score from sklearn.ensemble import RandomForestClassifier np.random.seed(2020) X = heart_disease.drop(&quot;target&quot;, axis=1) y = heart_disease[&quot;target&quot;] clf = RandomForestClassifier() cross_val_score = cross_val_score(clf, X, y) . np.mean(cross_val_score) . 0.8149726775956283 . print(f&quot;Heart Disease Classifier Cross-Validated Accuracy: {np.mean(cross_val_score) *100:.2f}%&quot;) . Heart Disease Classifier Cross-Validated Accuracy: 81.50% . 3.2 Area under the receiver operating characteristic curve (AUC/ROC) . Area under curve (AUC) | ROC curve | . ROC curves are a comparison of a model&#39;s true positive rate (tpr) versus a model&#39;s false positive rate (fpr). . True positive = model predicts 1 when truth is 1 | False positive = model predicts 1 when truth is 0 | True negative = model predicts 0 when truth is 0 | False negative = model predicts 1 when truth is 1 | . # Split data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2) . from sklearn.metrics import roc_curve # Fit the classifier clf.fit(X_train, y_train) # Make predictions with probabilities y_probs = clf.predict_proba(X_test) y_probs[:10] . array([[0.25, 0.75], [0.19, 0.81], [0.76, 0.24], [0.99, 0.01], [0.46, 0.54], [0.36, 0.64], [0.01, 0.99], [0.21, 0.79], [0.45, 0.55], [0.08, 0.92]]) . y_probs_positive = y_probs[:, 1] y_probs_positive[:10] . array([0.75, 0.81, 0.24, 0.01, 0.54, 0.64, 0.99, 0.79, 0.55, 0.92]) . # Caculate fpr, tpr and thresholds fpr, tpr, thresholds = roc_curve(y_test, y_probs_positive) # Check the false positive rates fpr . array([0. , 0. , 0. , 0. , 0. , 0. , 0.03703704, 0.03703704, 0.03703704, 0.03703704, 0.07407407, 0.07407407, 0.07407407, 0.07407407, 0.11111111, 0.11111111, 0.14814815, 0.18518519, 0.18518519, 0.22222222, 0.22222222, 0.2962963 , 0.33333333, 0.33333333, 0.33333333, 0.37037037, 0.37037037, 0.44444444, 0.51851852, 0.51851852, 0.59259259, 0.7037037 , 0.74074074, 0.81481481, 0.88888889, 0.96296296, 1. ]) . # Create a function for plotting ROC curves def plot_roc_curve(fpr, tpr): &quot;&quot;&quot; Plots a ROC curve given the false positive rate (fpr) and true positive rate (tpr) of a model. &quot;&quot;&quot; # Plot roc curve plt.plot(fpr, tpr, color=&quot;orange&quot;, label=&quot;ROC&quot;) # Plot line with no predictive power (baseline) plt.plot([0, 1], [0, 1], color=&quot;darkblue&quot;, linestyle=&quot;--&quot;, label=&quot;Guessing&quot;) # Customize the plot plt.xlabel(&quot;False positive rate (fpr)&quot;) plt.ylabel(&quot;True positive rate (tpr)&quot;) plt.title(&quot;Receiver Operating Characteristic (ROC) Curve&quot;) plt.legend() plt.show() plot_roc_curve(fpr, tpr) . from sklearn.metrics import roc_auc_score roc_auc_score(y_test, y_probs_positive) . 0.8921568627450981 . # Plot perfect ROC curve and AUC score fpr, tpr, thresholds = roc_curve(y_test, y_test) plot_roc_curve(fpr, tpr) . # Perfect AUC score roc_auc_score(y_test, y_test) . 1.0 . 3.3 Confusion Matrix . A confusion matrix is a quick way to compare the labels a model predicts and the actual labels it was supposed to predict. . In essence, giving you an idea of where the model is getting confused. . from sklearn.metrics import confusion_matrix y_preds = clf.predict(X_test) confusion_matrix(y_test, y_preds) . array([[21, 6], [ 7, 27]], dtype=int64) . # Visualize confusion matrix with pd.crosstab() pd.crosstab(y_test, y_preds, rownames=[&quot;Actual Label&quot;], colnames=[&quot;Predicted Labels&quot;]) . Predicted Labels 0 1 . Actual Label . 0 21 | 6 | . 1 7 | 27 | . # How to install a conda package into the current environment from a Jupyter Notebook import sys !conda install --yes --prefix {sys.prefix} seaborn . Collecting package metadata (current_repodata.json): ...working... done . ==&gt; WARNING: A newer version of conda exists. &lt;== current version: 4.7.12 latest version: 4.8.3 Please update conda by running $ conda update -n base -c defaults conda . Solving environment: ...working... done # All requested packages already installed. . # Make our confusion matrix more visual with Seaborn&#39;s heatmap() import seaborn as sns # Set the font scale sns.set(font_scale=1.5) # Create a confusion matrix conf_mat = confusion_matrix(y_test, y_preds) # Plot it using Seaborn sns.heatmap(conf_mat); . def plot_conf_mat(conf_mat): &quot;&quot;&quot; Plot a confusion matrix using Seaborn&#39;s heatmap(). &quot;&quot;&quot; fig, ax = plt.subplots(figsize=(3,3)) ax = sns.heatmap(conf_mat, annot=True, # Annotate the boxes with conf_mat info cbar=False) plt.xlabel(&quot;True label&quot;) plt.ylabel(&quot;Predicted label&quot;); plot_conf_mat(conf_mat) . 3.4 Classification Report . from sklearn.metrics import classification_report print(classification_report(y_test, y_preds)) . precision recall f1-score support 0 0.75 0.78 0.76 27 1 0.82 0.79 0.81 34 accuracy 0.79 61 macro avg 0.78 0.79 0.78 61 weighted avg 0.79 0.79 0.79 61 . # Where precision and recall become valuable disease_true = np.zeros(10000) disease_true[0] = 1 # only one positive case disease_preds = np.zeros(10000) # model predicts every case as 0 pd.DataFrame(classification_report(disease_true, disease_preds, output_dict=True)) . E: machine_learning env lib site-packages sklearn metrics _classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior. _warn_prf(average, modifier, msg_start, len(result)) . 0.0 1.0 accuracy macro avg weighted avg . precision 0.99990 | 0.0 | 0.9999 | 0.499950 | 0.99980 | . recall 1.00000 | 0.0 | 0.9999 | 0.500000 | 0.99990 | . f1-score 0.99995 | 0.0 | 0.9999 | 0.499975 | 0.99985 | . support 9999.00000 | 1.0 | 0.9999 | 10000.000000 | 10000.00000 | . To summarize classification metrics: . Accuracy is a good measure to start with if all classes are balanced (e.g. same amount of samples which are labelled with 0 or 1). | Precision and recall become more important when classes are imbalanced. | If false positive predictions are worse than false negatives, aim for higher precision. | If false negative predictions are worse than false positives, aim for higher recall. | F1-score is a combination of precision and recall. | . 4. Regression model evaluation metrics . Model evaluation metrics documentation - https://scikit-learn.org/stable/modules/model_evaluation.html . R^2 (pronounced r-squared) or coefficient of determination. | Mean absolute error (MAE) | Mean squared error (MSE) | 4.1 R^2 . What R-squared does: . Compares your models predictions to the mean of the target. Values can range from negative infinity (a very pool model) to 1. For example, if all your model does is predict the mean of the targets, its R^2 value would be 0. But if your model perfectly predicts a range of numbers, its R^2 values would be 1. . from sklearn.ensemble import RandomForestRegressor np.random.seed(2020) X = boston_df.drop(&quot;target&quot;, axis=1) y = boston_df[&quot;target&quot;] X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2) model = RandomForestRegressor() model.fit(X_train, y_train); . model.score(X_test, y_test) . 0.7873300907872445 . from sklearn.metrics import r2_score # Fill an array with y_test mean y_test_mean = np.full(len(y_test), y_test.mean()) . y_test.mean() . 21.38823529411765 . r2_score(y_test, y_test_mean) . 0.0 . r2_score(y_test, y_test) . 1.0 . 4.2 Mean absolute error . MAE is the average of the absolute differences between predictions and actual values. It gives an idea of how wrong your model&#39;s predictions are. . # Mean absolute error from sklearn.metrics import mean_absolute_error y_preds = model.predict(X_test) mae = mean_absolute_error(y_test, y_preds) mae . 2.807558823529411 . df = pd.DataFrame(data={&quot;actual values&quot;: y_test, &quot;predicted values&quot;: y_preds}) df[&quot;differences&quot;] = df[&quot;predicted values&quot;] - df[&quot;actual values&quot;] df . actual values predicted values differences . 409 27.5 | 22.611 | -4.889 | . 247 20.5 | 21.114 | 0.614 | . 399 6.3 | 12.712 | 6.412 | . 300 24.8 | 29.828 | 5.028 | . 321 23.1 | 24.184 | 1.084 | . ... ... | ... | ... | . 204 50.0 | 47.663 | -2.337 | . 495 23.1 | 19.530 | -3.570 | . 244 17.6 | 18.785 | 1.185 | . 413 16.3 | 13.995 | -2.305 | . 216 23.3 | 21.388 | -1.912 | . 102 rows × 3 columns . 4.3 Mean squared error (MSE) . # Mean squared error from sklearn.metrics import mean_squared_error y_preds = model.predict(X_test) mse = mean_squared_error(y_test, y_preds) mse . 18.147552617647058 . # Calculate MSE by hand squared = np.square(df[&quot;differences&quot;]) squared.mean() . 18.147552617647058 . 4.4 Finally, using the scoring parameter . from sklearn.model_selection import cross_val_score from sklearn.ensemble import RandomForestClassifier np.random.seed(2020) X = heart_disease.drop(&quot;target&quot;, axis=1) y = heart_disease[&quot;target&quot;] clf = RandomForestClassifier() . np.random.seed(2020) cv_acc = cross_val_score(clf, X, y) cv_acc . array([0.80327869, 0.90163934, 0.80327869, 0.78333333, 0.78333333]) . # Cross-validated accuracy print(f&#39;The cross-validated accuracy is: {np.mean(cv_acc)*100:.2f}%&#39;) . The cross-validated accuracy is: 81.50% . np.random.seed(2020) cv_acc = cross_val_score(clf, X, y, scoring=&quot;accuracy&quot;) print(f&#39;The cross-validated accuracy is: {np.mean(cv_acc)*100:.2f}%&#39;) . The cross-validated accuracy is: 81.50% . # Precision cv_precision = cross_val_score(clf, X, y, scoring=&quot;precision&quot;) np.mean(cv_precision) . 0.8149098212659334 . # Recall cv_recall = cross_val_score(clf, X, y, scoring=&quot;recall&quot;) np.mean(cv_recall) . 0.8363636363636363 . cv_f1 = cross_val_score(clf, X, y, scoring=&quot;f1&quot;) np.mean(cv_f1) . 0.8557553692891091 . How about the regression model? . from sklearn.model_selection import cross_val_score from sklearn.ensemble import RandomForestRegressor np.random.seed(2020) X = boston_df.drop(&quot;target&quot;, axis=1) y = boston_df[&quot;target&quot;] model = RandomForestRegressor() . np.random.seed(2020) cv_r2 = cross_val_score(model, X, y, scoring=None) cv_r2 . array([0.7590384 , 0.85738467, 0.70144943, 0.45694151, 0.23590506]) . np.random.seed(2020) cv_r2 = cross_val_score(model, X, y, scoring=&quot;r2&quot;) cv_r2 . array([0.7590384 , 0.85738467, 0.70144943, 0.45694151, 0.23590506]) . # Mean absolute error cv_mae = cross_val_score(model, X, y, scoring=&quot;neg_mean_absolute_error&quot;) cv_mae . array([-2.12122549, -2.55272277, -3.58191089, -3.82661386, -3.06138614]) . # Mean sqaured error cv_mse = cross_val_score(model, X, y, scoring=&quot;neg_mean_squared_error&quot;) cv_mse . array([ -7.74356199, -13.29979953, -22.26308961, -45.66473461, -18.772259 ]) . Using different evaluation metrics as Scikit-Learn functions . Classification evaluation functions . from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import train_test_split np.random.seed(2020) X = heart_disease.drop(&quot;target&quot;, axis=1) y = heart_disease[&quot;target&quot;] X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2) clf = RandomForestClassifier() clf.fit(X_train, y_train) # Make some predictions y_preds = clf.predict(X_test) # Evaluate the classifier print(&quot;Classifier metrics on the test set&quot;) print(f&quot;Accuracy: {accuracy_score(y_test, y_preds)*100:.2f}%&quot;) print(f&quot;Precision: {precision_score(y_test, y_preds)*100:.2f}%&quot;) print(f&quot;Recall: {recall_score(y_test, y_preds)*100:.2f}%&quot;) print(f&quot;F1: {f1_score(y_test, y_preds)*100:.2f}%&quot;) . Classifier metrics on the test set Accuracy: 78.69% Precision: 78.95% Recall: 85.71% F1: 82.19% . Regression evaluation functions . from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error from sklearn.ensemble import RandomForestRegressor from sklearn.model_selection import train_test_split np.random.seed(2020) X = boston_df.drop(&quot;target&quot;, axis=1) y = boston_df[&quot;target&quot;] X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2) model = RandomForestRegressor() model.fit(X_train, y_train) # Make predictions using our regression model y_preds = model.predict(X_test) # Evaluate the regression model print(&quot;Regression model metrics on the test set&quot;) print(f&quot;R^2: {r2_score(y_test, y_preds)}&quot;) print(f&quot;MAE: {mean_absolute_error(y_test, y_preds)}&quot;) print(f&quot;MSE: {mean_squared_error(y_test, y_preds)}&quot;) . Regression model metrics on the test set R^2: 0.7873300907872445 MAE: 2.807558823529411 MSE: 18.147552617647058 .",
            "url": "https://www.ai4love.xyz//2020/04/12/2020-04-11-Evaluate-a-model.html",
            "relUrl": "/2020/04/12/2020-04-11-Evaluate-a-model.html",
            "date": " • Apr 12, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Fit the model/algorithm on our data and use it to make predictions",
            "content": "# Standard imports import pandas as pd import numpy as np import matplotlib.pyplot as plt %matplotlib inline . heart_disease = pd.read_csv(&quot;https://raw.githubusercontent.com/CongLiu-CN/zero-to-mastery-ml/master/data/heart-disease.csv&quot;) heart_disease.head() . age sex cp trestbps chol fbs restecg thalach exang oldpeak slope ca thal target . 0 63 | 1 | 3 | 145 | 233 | 1 | 0 | 150 | 0 | 2.3 | 0 | 0 | 1 | 1 | . 1 37 | 1 | 2 | 130 | 250 | 0 | 1 | 187 | 0 | 3.5 | 0 | 0 | 2 | 1 | . 2 41 | 0 | 1 | 130 | 204 | 0 | 0 | 172 | 0 | 1.4 | 2 | 0 | 2 | 1 | . 3 56 | 1 | 1 | 120 | 236 | 0 | 1 | 178 | 0 | 0.8 | 2 | 0 | 2 | 1 | . 4 57 | 0 | 0 | 120 | 354 | 0 | 1 | 163 | 1 | 0.6 | 2 | 0 | 2 | 1 | . 1. Fitting the model to the data . Different names for: . X = features, features variables, data | y = labels, targets, target variables | . # Import the RandomForestClassifier estimator class from sklearn.ensemble import RandomForestClassifier # Setup random seed np.random.seed(2020) # Make the data X = heart_disease.drop(&quot;target&quot;, axis=1) y = heart_disease[&quot;target&quot;] # Split the data from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2) # Instantiate Random Forest Classifier clf = RandomForestClassifier() # Fit the model to the data (training the machine learning model) clf.fit(X_train, y_train) # Evaluate the Random Forest Classifier (use the patterns the model has learned) clf.score(X_test, y_test) . 0.7868852459016393 . 2. Make predictions using a machine learning model . 2 ways to make predictions: . predict() | predict_proba() | # Use a trained model to make predictions clf.predict(X_test) . array([0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1], dtype=int64) . np.array(y_test) . array([0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0], dtype=int64) . # Compare predictions to truth labels to evaluate the model y_preds = clf.predict(X_test) np.mean(y_preds == y_test) . 0.7868852459016393 . clf.score(X_test, y_test) . 0.7868852459016393 . from sklearn.metrics import accuracy_score accuracy_score(y_test, y_preds) . 0.7868852459016393 . Make predictions with predict_proba(): . # predict_proba() returns probabilities of a classification label clf.predict_proba(X_test[:5]) . array([[0.77, 0.23], [0.31, 0.69], [0.73, 0.27], [0.48, 0.52], [0.07, 0.93]]) . # Let&#39;s predict() on the same data... clf.predict(X_test[:5]) . array([0, 1, 0, 1, 1], dtype=int64) . predict() can also be used for regression models. . # Import Boston housing dataset from sklearn.datasets import load_boston boston = load_boston() boston; . # Turn boston into a pandas dataframe boston_df = pd.DataFrame(boston[&quot;data&quot;], columns=boston[&quot;feature_names&quot;]) boston_df[&quot;target&quot;] = pd.Series(boston[&quot;target&quot;]) boston_df.head() . CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX PTRATIO B LSTAT target . 0 0.00632 | 18.0 | 2.31 | 0.0 | 0.538 | 6.575 | 65.2 | 4.0900 | 1.0 | 296.0 | 15.3 | 396.90 | 4.98 | 24.0 | . 1 0.02731 | 0.0 | 7.07 | 0.0 | 0.469 | 6.421 | 78.9 | 4.9671 | 2.0 | 242.0 | 17.8 | 396.90 | 9.14 | 21.6 | . 2 0.02729 | 0.0 | 7.07 | 0.0 | 0.469 | 7.185 | 61.1 | 4.9671 | 2.0 | 242.0 | 17.8 | 392.83 | 4.03 | 34.7 | . 3 0.03237 | 0.0 | 2.18 | 0.0 | 0.458 | 6.998 | 45.8 | 6.0622 | 3.0 | 222.0 | 18.7 | 394.63 | 2.94 | 33.4 | . 4 0.06905 | 0.0 | 2.18 | 0.0 | 0.458 | 7.147 | 54.2 | 6.0622 | 3.0 | 222.0 | 18.7 | 396.90 | 5.33 | 36.2 | . from sklearn.ensemble import RandomForestRegressor np.random.seed(2020) # Create the data X = boston_df.drop(&quot;target&quot;, axis=1) y = boston_df[&quot;target&quot;] # Split into training and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2) # Instantiate and fit model model = RandomForestRegressor().fit(X_train, y_train) # Make predictions y_preds = model.predict(X_test) . y_preds[:10] . array([22.611, 21.114, 12.712, 29.828, 24.184, 19.73 , 23.794, 35.071, 12.503, 23.377]) . np.array(y_test[:10]) . array([27.5, 20.5, 6.3, 24.8, 23.1, 14.5, 16.5, 15. , 10.2, 22.3]) . # Compare the predictions to the truth from sklearn.metrics import mean_absolute_error mean_absolute_error(y_test, y_preds) . 2.807558823529411 .",
            "url": "https://www.ai4love.xyz//2020/04/10/Fit-a-model-to-the-data.html",
            "relUrl": "/2020/04/10/Fit-a-model-to-the-data.html",
            "date": " • Apr 10, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Choosing the right estimator/algorithm for our problem",
            "content": "# Standard imports import pandas as pd import numpy as np import matplotlib.pyplot as plt %matplotlib inline . 1. Picking a machine learning model for a regression problem . # Import Boston housing dataset from sklearn.datasets import load_boston boston = load_boston() boston; . Turn boston into a pandas dataframe: . boston_df = pd.DataFrame(boston[&quot;data&quot;], columns=boston[&quot;feature_names&quot;]) boston_df[&quot;target&quot;] = pd.Series(boston[&quot;target&quot;]) boston_df.head() . CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX PTRATIO B LSTAT target . 0 0.00632 | 18.0 | 2.31 | 0.0 | 0.538 | 6.575 | 65.2 | 4.0900 | 1.0 | 296.0 | 15.3 | 396.90 | 4.98 | 24.0 | . 1 0.02731 | 0.0 | 7.07 | 0.0 | 0.469 | 6.421 | 78.9 | 4.9671 | 2.0 | 242.0 | 17.8 | 396.90 | 9.14 | 21.6 | . 2 0.02729 | 0.0 | 7.07 | 0.0 | 0.469 | 7.185 | 61.1 | 4.9671 | 2.0 | 242.0 | 17.8 | 392.83 | 4.03 | 34.7 | . 3 0.03237 | 0.0 | 2.18 | 0.0 | 0.458 | 6.998 | 45.8 | 6.0622 | 3.0 | 222.0 | 18.7 | 394.63 | 2.94 | 33.4 | . 4 0.06905 | 0.0 | 2.18 | 0.0 | 0.458 | 7.147 | 54.2 | 6.0622 | 3.0 | 222.0 | 18.7 | 396.90 | 5.33 | 36.2 | . # How many samples? len(boston_df) . 506 . Let&#39;s try the Ridge Regression model: . from sklearn.linear_model import Ridge # Setup random seed np.random.seed(2020) # Create the data X = boston_df.drop(&quot;target&quot;, axis=1) y = boston_df[&quot;target&quot;] # Split into train and test sets from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2) # Instantiate Ridge model model = Ridge() model.fit(X_train, y_train) # Check the score of the Ridge model on test data model.score(X_test, y_test) . 0.7635674632126357 . How do we improve this model? . What if Ridge wasn&#39;t working? . Let&#39;s refer back to the map...https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html . Let&#39;s try the Random Forest Regressor: . from sklearn.ensemble import RandomForestRegressor # Setup random seed np.random.seed(2020) # Create the data X = boston_df.drop(&quot;target&quot;, axis=1) y = boston_df[&quot;target&quot;] # Split the data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2) # Instantiate Random Forest Regressor rf = RandomForestRegressor() rf.fit(X_train, y_train) #Evaluate the Random Forest Regressor rf.score(X_test, y_test) . 0.7873300907872445 . 2. Choosing an estimator for a classification problem . Let&#39;s go to the map... https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html . heart_disease = pd.read_csv(&quot;https://raw.githubusercontent.com/CongLiu-CN/zero-to-mastery-ml/master/data/heart-disease.csv&quot;) heart_disease.head() . age sex cp trestbps chol fbs restecg thalach exang oldpeak slope ca thal target . 0 63 | 1 | 3 | 145 | 233 | 1 | 0 | 150 | 0 | 2.3 | 0 | 0 | 1 | 1 | . 1 37 | 1 | 2 | 130 | 250 | 0 | 1 | 187 | 0 | 3.5 | 0 | 0 | 2 | 1 | . 2 41 | 0 | 1 | 130 | 204 | 0 | 0 | 172 | 0 | 1.4 | 2 | 0 | 2 | 1 | . 3 56 | 1 | 1 | 120 | 236 | 0 | 1 | 178 | 0 | 0.8 | 2 | 0 | 2 | 1 | . 4 57 | 0 | 0 | 120 | 354 | 0 | 1 | 163 | 1 | 0.6 | 2 | 0 | 2 | 1 | . len(heart_disease) . 303 . Consulting the map and it says to try LinearSVC. . # Import the LinearSVC estimator class from sklearn.svm import LinearSVC # Setup random seed np.random.seed(2020) # Make the data X = heart_disease.drop(&quot;target&quot;, axis=1) y = heart_disease[&quot;target&quot;] # Split the data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2) # Instantiate LinearSVC clf = LinearSVC(max_iter=100000) clf.fit(X_train, y_train) # Evaluate the LinearSVC clf.score(X_test, y_test) . E: machine_learning env lib site-packages sklearn svm _base.py:946: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations. warnings.warn(&#34;Liblinear failed to converge, increase &#34; . 0.7540983606557377 . # Import the RandomForestClassifier estimator class from sklearn.ensemble import RandomForestClassifier # Setup random seed np.random.seed(2020) # Make the data X = heart_disease.drop(&quot;target&quot;, axis=1) y = heart_disease[&quot;target&quot;] # Split the data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2) # Instantiate Random Forest Classifier clf = RandomForestClassifier() clf.fit(X_train, y_train) # Evaluate the Random Forest Classifier clf.score(X_test, y_test) . 0.7868852459016393 . Tidbit: . If you have structured data, use ensemble methods | If you have unstructured data, use deep learning or transfer learning |",
            "url": "https://www.ai4love.xyz//2020/04/06/Choosing-the-right-estimator.html",
            "relUrl": "/2020/04/06/Choosing-the-right-estimator.html",
            "date": " • Apr 6, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Getting our data ready to be used with machine learning",
            "content": "# Standard imports import pandas as pd import numpy as np import matplotlib.pyplot as plt %matplotlib inline . Three main things we have to do: . 1. Splitting the data into features and labels (usually `X` &amp; `y`) 2. Filling (also called imputing) or disregarding missing values 3. Converting non-numerical values to numerical values (also called feature encoding) . Let&#39;s still use our heart disease data by importing it from Github: . heart_disease = pd.read_csv(&quot;https://raw.githubusercontent.com/CongLiu-CN/zero-to-mastery-ml/master/data/heart-disease.csv&quot;) heart_disease.head() . age sex cp trestbps chol fbs restecg thalach exang oldpeak slope ca thal target . 0 63 | 1 | 3 | 145 | 233 | 1 | 0 | 150 | 0 | 2.3 | 0 | 0 | 1 | 1 | . 1 37 | 1 | 2 | 130 | 250 | 0 | 1 | 187 | 0 | 3.5 | 0 | 0 | 2 | 1 | . 2 41 | 0 | 1 | 130 | 204 | 0 | 0 | 172 | 0 | 1.4 | 2 | 0 | 2 | 1 | . 3 56 | 1 | 1 | 120 | 236 | 0 | 1 | 178 | 0 | 0.8 | 2 | 0 | 2 | 1 | . 4 57 | 0 | 0 | 120 | 354 | 0 | 1 | 163 | 1 | 0.6 | 2 | 0 | 2 | 1 | . X = heart_disease.drop(&quot;target&quot;, axis=1) X.head() . age sex cp trestbps chol fbs restecg thalach exang oldpeak slope ca thal . 0 63 | 1 | 3 | 145 | 233 | 1 | 0 | 150 | 0 | 2.3 | 0 | 0 | 1 | . 1 37 | 1 | 2 | 130 | 250 | 0 | 1 | 187 | 0 | 3.5 | 0 | 0 | 2 | . 2 41 | 0 | 1 | 130 | 204 | 0 | 0 | 172 | 0 | 1.4 | 2 | 0 | 2 | . 3 56 | 1 | 1 | 120 | 236 | 0 | 1 | 178 | 0 | 0.8 | 2 | 0 | 2 | . 4 57 | 0 | 0 | 120 | 354 | 0 | 1 | 163 | 1 | 0.6 | 2 | 0 | 2 | . y = heart_disease[&quot;target&quot;] y.head() . 0 1 1 1 2 1 3 1 4 1 Name: target, dtype: int64 . 1. Split the data into training and test sets: . from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2) X_train.shape, X_test.shape, y_train.shape, y_test.shape . ((242, 13), (61, 13), (242,), (61,)) . X.shape[0] * 0.8, X_train.shape[0] . (242.4, 242) . 242 + 61 == len(heart_disease) . True . 2. Make sure it&#39;s all numerical . car_sales = pd.read_csv(&quot;https://raw.githubusercontent.com/CongLiu-CN/zero-to-mastery-ml/master/data/car-sales-extended.csv&quot;) car_sales.head() . Make Colour Odometer (KM) Doors Price . 0 Honda | White | 35431 | 4 | 15323 | . 1 BMW | Blue | 192714 | 5 | 19943 | . 2 Honda | White | 84714 | 4 | 28343 | . 3 Toyota | White | 154365 | 4 | 13434 | . 4 Nissan | Blue | 181577 | 3 | 14043 | . len(car_sales) . 1000 . car_sales.dtypes . Make object Colour object Odometer (KM) int64 Doors int64 Price int64 dtype: object . Split the data into X &amp; y: . X = car_sales.drop(&quot;Price&quot;, axis=1) y = car_sales[&quot;Price&quot;] . Split the data into training &amp; test: . X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2) . Build machine learning model: . from sklearn.ensemble import RandomForestRegressor model = RandomForestRegressor() model.fit(X_train, y_train) model.score(X_test, y_test) . - ValueError Traceback (most recent call last) &lt;ipython-input-13-4b70515c7cdf&gt; in &lt;module&gt; 2 3 model = RandomForestRegressor() -&gt; 4 model.fit(X_train, y_train) 5 model.score(X_test, y_test) E: machine_learning env lib site-packages sklearn ensemble _forest.py in fit(self, X, y, sample_weight) 293 &#34;&#34;&#34; 294 # Validate or convert input data --&gt; 295 X = check_array(X, accept_sparse=&#34;csc&#34;, dtype=DTYPE) 296 y = check_array(y, accept_sparse=&#39;csc&#39;, ensure_2d=False, dtype=None) 297 if sample_weight is not None: E: machine_learning env lib site-packages sklearn utils validation.py in check_array(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator) 529 array = array.astype(dtype, casting=&#34;unsafe&#34;, copy=False) 530 else: --&gt; 531 array = np.asarray(array, order=order, dtype=dtype) 532 except ComplexWarning: 533 raise ValueError(&#34;Complex data not supported n&#34; E: machine_learning env lib site-packages numpy core _asarray.py in asarray(a, dtype, order) 83 84 &#34;&#34;&#34; &gt; 85 return array(a, dtype, copy=False, order=order) 86 87 ValueError: could not convert string to float: &#39;BMW&#39; . Machine learning model can&#39;t handle strings, so we need to convert those strings into numbers. . Turn the categories into numbers: . from sklearn.preprocessing import OneHotEncoder from sklearn.compose import ColumnTransformer categorical_features = [&quot;Make&quot;, &quot;Colour&quot;, &quot;Doors&quot;] one_hot = OneHotEncoder() transformer = ColumnTransformer([(&quot;one_hot&quot;, one_hot, categorical_features)], remainder=&quot;passthrough&quot;) transformed_X = transformer.fit_transform(X) transformed_X . array([[0.00000e+00, 1.00000e+00, 0.00000e+00, ..., 1.00000e+00, 0.00000e+00, 3.54310e+04], [1.00000e+00, 0.00000e+00, 0.00000e+00, ..., 0.00000e+00, 1.00000e+00, 1.92714e+05], [0.00000e+00, 1.00000e+00, 0.00000e+00, ..., 1.00000e+00, 0.00000e+00, 8.47140e+04], ..., [0.00000e+00, 0.00000e+00, 1.00000e+00, ..., 1.00000e+00, 0.00000e+00, 6.66040e+04], [0.00000e+00, 1.00000e+00, 0.00000e+00, ..., 1.00000e+00, 0.00000e+00, 2.15883e+05], [0.00000e+00, 0.00000e+00, 0.00000e+00, ..., 1.00000e+00, 0.00000e+00, 2.48360e+05]]) . pd.DataFrame(transformed_X) . 0 1 2 3 4 5 6 7 8 9 10 11 12 . 0 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 35431.0 | . 1 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 192714.0 | . 2 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 84714.0 | . 3 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 154365.0 | . 4 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 181577.0 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 995 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 35820.0 | . 996 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 0.0 | 0.0 | 155144.0 | . 997 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 66604.0 | . 998 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 215883.0 | . 999 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 248360.0 | . 1000 rows × 13 columns . What OneHotEncoder does under the hood: . . dummies = pd.get_dummies(car_sales[[&quot;Make&quot;, &quot;Colour&quot;, &quot;Doors&quot;]]) dummies . Doors Make_BMW Make_Honda Make_Nissan Make_Toyota Colour_Black Colour_Blue Colour_Green Colour_Red Colour_White . 0 4 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | . 1 5 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | . 2 4 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | . 3 4 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | . 4 3 | 0 | 0 | 1 | 0 | 0 | 1 | 0 | 0 | 0 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 995 4 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | . 996 3 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 1 | . 997 4 | 0 | 0 | 1 | 0 | 0 | 1 | 0 | 0 | 0 | . 998 4 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | . 999 4 | 0 | 0 | 0 | 1 | 0 | 1 | 0 | 0 | 0 | . 1000 rows × 10 columns . Let&#39;s refit the model: . np.random.seed(2020) X_train, X_test, y_train, y_test = train_test_split(transformed_X, y, test_size=0.2) model.fit(X_train, y_train) . RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion=&#39;mse&#39;, max_depth=None, max_features=&#39;auto&#39;, max_leaf_nodes=None, max_samples=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None, oob_score=False, random_state=None, verbose=0, warm_start=False) . model.score(X_test, y_test) . 0.19383319494749351 . Why the score is so low? . --Intentionally blank line-- . 3. What if there were missing values? . Fill them with some value (also known as imputation). | Remove the samples with missing data altogether. | This time what we import is the car sales data with missing values: . car_sales_missing = pd.read_csv(&quot;https://raw.githubusercontent.com/CongLiu-CN/zero-to-mastery-ml/master/data/car-sales-extended-missing-data.csv&quot;) car_sales_missing.head() . Make Colour Odometer (KM) Doors Price . 0 Honda | White | 35431.0 | 4.0 | 15323.0 | . 1 BMW | Blue | 192714.0 | 5.0 | 19943.0 | . 2 Honda | White | 84714.0 | 4.0 | 28343.0 | . 3 Toyota | White | 154365.0 | 4.0 | 13434.0 | . 4 Nissan | Blue | 181577.0 | 3.0 | 14043.0 | . car_sales_missing.isna().sum() . Make 49 Colour 50 Odometer (KM) 50 Doors 50 Price 50 dtype: int64 . Create X &amp; y: . X = car_sales_missing.drop(&quot;Price&quot;, axis=1) y = car_sales_missing[&quot;Price&quot;] . Let&#39;s try and convert our data to numbers. . Turn the categories into numbers: . from sklearn.preprocessing import OneHotEncoder from sklearn.compose import ColumnTransformer categorical_features = [&quot;Make&quot;, &quot;Colour&quot;, &quot;Doors&quot;] one_hot = OneHotEncoder() transformer = ColumnTransformer([(&quot;one_hot&quot;, one_hot, categorical_features)], remainder=&quot;passthrough&quot;) transformed_X = transformer.fit_transform(X) transformed_X . - ValueError Traceback (most recent call last) &lt;ipython-input-22-c22fbfad0141&gt; in &lt;module&gt; 9 remainder=&#34;passthrough&#34;) 10 &gt; 11 transformed_X = transformer.fit_transform(X) 12 transformed_X E: machine_learning env lib site-packages sklearn compose _column_transformer.py in fit_transform(self, X, y) 516 self._validate_remainder(X) 517 --&gt; 518 result = self._fit_transform(X, y, _fit_transform_one) 519 520 if not result: E: machine_learning env lib site-packages sklearn compose _column_transformer.py in _fit_transform(self, X, y, func, fitted) 446 self._iter(fitted=fitted, replace_strings=True)) 447 try: --&gt; 448 return Parallel(n_jobs=self.n_jobs)( 449 delayed(func)( 450 transformer=clone(trans) if not fitted else trans, E: machine_learning env lib site-packages joblib parallel.py in __call__(self, iterable) 1002 # remaining jobs. 1003 self._iterating = False -&gt; 1004 if self.dispatch_one_batch(iterator): 1005 self._iterating = self._original_iterator is not None 1006 E: machine_learning env lib site-packages joblib parallel.py in dispatch_one_batch(self, iterator) 833 return False 834 else: --&gt; 835 self._dispatch(tasks) 836 return True 837 E: machine_learning env lib site-packages joblib parallel.py in _dispatch(self, batch) 752 with self._lock: 753 job_idx = len(self._jobs) --&gt; 754 job = self._backend.apply_async(batch, callback=cb) 755 # A job can complete so quickly than its callback is 756 # called before we get here, causing self._jobs to E: machine_learning env lib site-packages joblib _parallel_backends.py in apply_async(self, func, callback) 207 def apply_async(self, func, callback=None): 208 &#34;&#34;&#34;Schedule a func to be run&#34;&#34;&#34; --&gt; 209 result = ImmediateResult(func) 210 if callback: 211 callback(result) E: machine_learning env lib site-packages joblib _parallel_backends.py in __init__(self, batch) 588 # Don&#39;t delay the application, to avoid keeping the input 589 # arguments in memory --&gt; 590 self.results = batch() 591 592 def get(self): E: machine_learning env lib site-packages joblib parallel.py in __call__(self) 253 # change the default number of processes to -1 254 with parallel_backend(self._backend, n_jobs=self._n_jobs): --&gt; 255 return [func(*args, **kwargs) 256 for func, args, kwargs in self.items] 257 E: machine_learning env lib site-packages joblib parallel.py in &lt;listcomp&gt;(.0) 253 # change the default number of processes to -1 254 with parallel_backend(self._backend, n_jobs=self._n_jobs): --&gt; 255 return [func(*args, **kwargs) 256 for func, args, kwargs in self.items] 257 E: machine_learning env lib site-packages sklearn pipeline.py in _fit_transform_one(transformer, X, y, weight, message_clsname, message, **fit_params) 726 with _print_elapsed_time(message_clsname, message): 727 if hasattr(transformer, &#39;fit_transform&#39;): --&gt; 728 res = transformer.fit_transform(X, y, **fit_params) 729 else: 730 res = transformer.fit(X, y, **fit_params).transform(X) E: machine_learning env lib site-packages sklearn preprocessing _encoders.py in fit_transform(self, X, y) 370 &#34;&#34;&#34; 371 self._validate_keywords() --&gt; 372 return super().fit_transform(X, y) 373 374 def transform(self, X): E: machine_learning env lib site-packages sklearn base.py in fit_transform(self, X, y, **fit_params) 569 if y is None: 570 # fit method of arity 1 (unsupervised transformation) --&gt; 571 return self.fit(X, **fit_params).transform(X) 572 else: 573 # fit method of arity 2 (supervised transformation) E: machine_learning env lib site-packages sklearn preprocessing _encoders.py in fit(self, X, y) 345 &#34;&#34;&#34; 346 self._validate_keywords() --&gt; 347 self._fit(X, handle_unknown=self.handle_unknown) 348 self.drop_idx_ = self._compute_drop_idx() 349 return self E: machine_learning env lib site-packages sklearn preprocessing _encoders.py in _fit(self, X, handle_unknown) 72 73 def _fit(self, X, handle_unknown=&#39;error&#39;): &gt; 74 X_list, n_samples, n_features = self._check_X(X) 75 76 if self.categories != &#39;auto&#39;: E: machine_learning env lib site-packages sklearn preprocessing _encoders.py in _check_X(self, X) 58 for i in range(n_features): 59 Xi = self._get_feature(X, feature_idx=i) &gt; 60 Xi = check_array(Xi, ensure_2d=False, dtype=None, 61 force_all_finite=needs_validation) 62 X_columns.append(Xi) E: machine_learning env lib site-packages sklearn utils validation.py in check_array(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator) 575 576 if force_all_finite: --&gt; 577 _assert_all_finite(array, 578 allow_nan=force_all_finite == &#39;allow-nan&#39;) 579 E: machine_learning env lib site-packages sklearn utils validation.py in _assert_all_finite(X, allow_nan, msg_dtype) 63 elif X.dtype == np.dtype(&#39;object&#39;) and not allow_nan: 64 if _object_dtype_isnan(X).any(): &gt; 65 raise ValueError(&#34;Input contains NaN&#34;) 66 67 ValueError: Input contains NaN . Why there is an error? . --Intentionally blank line-- . car_sales_missing[&quot;Doors&quot;].value_counts() . 4.0 811 5.0 75 3.0 64 Name: Doors, dtype: int64 . Option 1: Fill missing data with Pandas . Learn more about filling missing values with pandas: https://www.geeksforgeeks.org/python-pandas-dataframe-fillna-to-replace-null-values-in-dataframe/ . # Fill the &quot;Make&quot; column car_sales_missing[&quot;Make&quot;].fillna(&quot;missing&quot;, inplace=True) # Fill the &quot;Colour&quot; column car_sales_missing[&quot;Colour&quot;].fillna(&quot;missing&quot;, inplace=True) # Fill the &quot;Odometer (KM)&quot; column car_sales_missing[&quot;Odometer (KM)&quot;].fillna(car_sales_missing[&quot;Odometer (KM)&quot;].mean(), inplace=True) # Fill the &quot;Doors&quot; column car_sales_missing[&quot;Doors&quot;].fillna(4, inplace=True) . # Check our dataframe again car_sales_missing.isna().sum() . Make 0 Colour 0 Odometer (KM) 0 Doors 0 Price 50 dtype: int64 . # Remove rows with missing Price value car_sales_missing.dropna(inplace=True) . car_sales_missing.isna().sum() . Make 0 Colour 0 Odometer (KM) 0 Doors 0 Price 0 dtype: int64 . Now we have lost 50 data because of the removed data in &quot;Price&quot; column. . len(car_sales_missing) . 950 . The missing values are all filled or removed, so we can convert them. . X = car_sales_missing.drop(&quot;Price&quot;, axis=1) y = car_sales_missing[&quot;Price&quot;] . Let&#39;s try and convert our data to numbers. Turn the categories into numbers: . from sklearn.preprocessing import OneHotEncoder from sklearn.compose import ColumnTransformer categorical_features = [&quot;Make&quot;, &quot;Colour&quot;, &quot;Doors&quot;] one_hot = OneHotEncoder() transformer = ColumnTransformer([(&quot;one_hot&quot;, one_hot, categorical_features)], remainder=&quot;passthrough&quot;) transformed_X = transformer.fit_transform(car_sales_missing) transformed_X . array([[0.00000e+00, 1.00000e+00, 0.00000e+00, ..., 0.00000e+00, 3.54310e+04, 1.53230e+04], [1.00000e+00, 0.00000e+00, 0.00000e+00, ..., 1.00000e+00, 1.92714e+05, 1.99430e+04], [0.00000e+00, 1.00000e+00, 0.00000e+00, ..., 0.00000e+00, 8.47140e+04, 2.83430e+04], ..., [0.00000e+00, 0.00000e+00, 1.00000e+00, ..., 0.00000e+00, 6.66040e+04, 3.15700e+04], [0.00000e+00, 1.00000e+00, 0.00000e+00, ..., 0.00000e+00, 2.15883e+05, 4.00100e+03], [0.00000e+00, 0.00000e+00, 0.00000e+00, ..., 0.00000e+00, 2.48360e+05, 1.27320e+04]]) . Option 2: Filling missing data and transforming categorical data with Scikit-Learn . The main takeaways: . Split your data first, always keep your training &amp; test data separate | Fill/transform the training set and test sets separately | . # standard imports import numpy as np import pandas as pd import matplotlib.pyplot as plt %matplotlib inline . car_sales_missing = pd.read_csv(&quot;https://raw.githubusercontent.com/CongLiu-CN/zero-to-mastery-ml/master/data/car-sales-extended-missing-data.csv&quot;) car_sales_missing.head() . Make Colour Odometer (KM) Doors Price . 0 Honda | White | 35431.0 | 4.0 | 15323.0 | . 1 BMW | Blue | 192714.0 | 5.0 | 19943.0 | . 2 Honda | White | 84714.0 | 4.0 | 28343.0 | . 3 Toyota | White | 154365.0 | 4.0 | 13434.0 | . 4 Nissan | Blue | 181577.0 | 3.0 | 14043.0 | . # Check missing values car_sales_missing.isna().sum() . Make 49 Colour 50 Odometer (KM) 50 Doors 50 Price 50 dtype: int64 . # Drop the rows with no labels car_sales_missing.dropna(subset=[&quot;Price&quot;], inplace=True) car_sales_missing.isna().sum() . Make 47 Colour 46 Odometer (KM) 48 Doors 47 Price 0 dtype: int64 . Note the data is split before any filling or transformations. . from sklearn.model_selection import train_test_split # Split into X &amp; y X = car_sales_missing.drop(&quot;Price&quot;, axis=1) y = car_sales_missing[&quot;Price&quot;] # Split data into train and test np.random.seed(2020) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2) . # Check missing values X.isna().sum() . Make 47 Colour 46 Odometer (KM) 48 Doors 47 dtype: int64 . Let&#39;s fill the missing values. . We must fill the training and test values separately to ensure training data stays with the training dta and test data stays with the test data. . Note: We use fit_transform() on the training data and transform() on the testing data. In essence, we learn the patterns in the training set and transform it via imputation (fit, then transform). Then we take those same patterns and fill the test set (transform only). . # Fill missing values with Scikit-Learn from sklearn.impute import SimpleImputer from sklearn.compose import ColumnTransformer # Fill categorical values with &#39;missing&#39; &amp; numerical values with mean cat_imputer = SimpleImputer(strategy=&quot;constant&quot;, fill_value=&quot;missing&quot;) door_imputer = SimpleImputer(strategy=&quot;constant&quot;, fill_value=4) num_imputer = SimpleImputer(strategy=&quot;mean&quot;) # Define columns cat_features = [&quot;Make&quot;, &quot;Colour&quot;] door_feature = [&quot;Doors&quot;] num_features = [&quot;Odometer (KM)&quot;] # Create an imputer (something that fills missing data) imputer = ColumnTransformer([ (&quot;cat_imputer&quot;, cat_imputer, cat_features), (&quot;door_imputer&quot;, door_imputer, door_feature), (&quot;num_imputer&quot;, num_imputer, num_features) ]) # Fill train and test values separately filled_X_train = imputer.fit_transform(X_train) filled_X_test = imputer.transform(X_test) # Check filled X_train filled_X_train . array([[&#39;Nissan&#39;, &#39;Blue&#39;, 3.0, 24219.0], [&#39;Honda&#39;, &#39;Blue&#39;, 4.0, 219217.0], [&#39;Nissan&#39;, &#39;White&#39;, 4.0, 152215.0], ..., [&#39;Toyota&#39;, &#39;White&#39;, 4.0, 112292.0], [&#39;Toyota&#39;, &#39;White&#39;, 4.0, 83594.0], [&#39;Nissan&#39;, &#39;White&#39;, 3.0, 131296.40416666667]], dtype=object) . Now we&#39;ve filled our missing values, let&#39;s check how many are missing from each set. (Should be 0) . # Get our transformed data arrays back into DataFrames car_sales_filled_train = pd.DataFrame(filled_X_train, columns=[&quot;Make&quot;, &quot;Colour&quot;, &quot;Doors&quot;, &quot;Odometer (KM)&quot;]) car_sales_filled_test = pd.DataFrame(filled_X_test, columns=[&quot;Make&quot;, &quot;Colour&quot;, &quot;Doors&quot;, &quot;Odometer (KM)&quot;]) # Check missing data in training set car_sales_filled_train.isna().sum() . Make 0 Colour 0 Doors 0 Odometer (KM) 0 dtype: int64 . # Check missing data in test set car_sales_filled_test.isna().sum() . Make 0 Colour 0 Doors 0 Odometer (KM) 0 dtype: int64 . Look! No more missing values! . # Check to see the original... still missing values car_sales_missing.isna().sum() . Make 47 Colour 46 Odometer (KM) 48 Doors 47 Price 0 dtype: int64 . Ok, no more missing values but we&#39;ve still got to turn our data into numbers. . Let&#39;s do that using one hot encoding. . Again, keeping our training and test data separate. . # Import OneHotEncoder class from sklearn from sklearn.preprocessing import OneHotEncoder # Now let&#39;s one hot encode the features with the same code as before categorical_features = [&quot;Make&quot;, &quot;Colour&quot;, &quot;Doors&quot;] one_hot = OneHotEncoder() transformer = ColumnTransformer([(&quot;one_hot&quot;, one_hot, categorical_features)], remainder=&quot;passthrough&quot;) # Fill train and test values separately transformed_X_train = transformer.fit_transform(car_sales_filled_train) transformed_X_test = transformer.transform(car_sales_filled_test) # Check transformed and filled X_train transformed_X_train.toarray() . array([[0.00000000e+00, 0.00000000e+00, 1.00000000e+00, ..., 0.00000000e+00, 0.00000000e+00, 2.42190000e+04], [0.00000000e+00, 1.00000000e+00, 0.00000000e+00, ..., 1.00000000e+00, 0.00000000e+00, 2.19217000e+05], [0.00000000e+00, 0.00000000e+00, 1.00000000e+00, ..., 1.00000000e+00, 0.00000000e+00, 1.52215000e+05], ..., [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ..., 1.00000000e+00, 0.00000000e+00, 1.12292000e+05], [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ..., 1.00000000e+00, 0.00000000e+00, 8.35940000e+04], [0.00000000e+00, 0.00000000e+00, 1.00000000e+00, ..., 0.00000000e+00, 0.00000000e+00, 1.31296404e+05]]) . Fit a model . Wonderful! Now we&#39;ve filled and transformed our data, ensuring the training and test sets have been kept separate. Let&#39;s fit a model to the training set and evaluate it on the test set. . # Since we&#39;ve transformed X, let&#39;s see if we can fit a model np.random.seed(2020) from sklearn.ensemble import RandomForestRegressor # Setup model model = RandomForestRegressor() # Make sure to use transformed (filled and one-hot encoded X data) model.fit(transformed_X_train, y_train) model.score(transformed_X_test, y_test) . 0.07839674412816233 . Why the score is even lower than the previous one? . --Becaue of the data we lost-- . len(car_sales), len(car_sales_missing) . (1000, 950) . The process of filling missing values is called Imputation. . The process of turn non-numerical values into numerical values is called Feature Engineering or Fearure Encoding. .",
            "url": "https://www.ai4love.xyz//2020/04/04/Splitting-Your-Data.html",
            "relUrl": "/2020/04/04/Splitting-Your-Data.html",
            "date": " • Apr 4, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Introduction to Scikit-Learn",
            "content": "This notebook records the 8.1 lesson of Daniel&#39;s ML course. He provides a summary of the chapter 8, which is listed below: . An end-to-end Scikit-Learn workflow | Getting the data ready | Choose the right estimator/algorithm for our problems | Fit the model/algorithm and use it to make predictions on our data | Evaluating a model | Improve a model | Save and load a trained model | Putting it all together | In this lesson, we focus only on the first section—An end-to-end Scikit-Learn workflow. This section is a brief overview of the entire chapter 8. Therefore, the content of this section can be a bit overwelming. There is no need to fully understand every points in this part. Its main purpose is only give students an opportunity to preview what they are going to learn later to better understand and use sklearn. . Daniel, the instructor of this course, shows every step of a common sklearn workflow in a simple version. Those steps are &quot;1. GET THE DATA READY&quot;, &quot;2. CHOOSE THE RIGHT MODEL AND HYPERPARAMETERS&quot;, &quot;3. FIT THE MODEL TO THE TRAINING DATA&quot;, &quot;4. EVALUATE THE MODEL ON THE TRAINING DATA AND TEST DATA&quot;, &quot;5. IMPROVE A MODEL&quot;, &quot;6. SAVE A MODEL AND LOAD IT&quot;. The last step in the list above is missing for unknown reasons. . 1. GET THE DATA READY . The first thing we need to do is not importing the data, but rather, importing some common libraries, such as pandas, numpy, matplotlib, and sklearn. But for now, let&#39;s just import the first two. . import pandas as pd import numpy as np . You may ask what the hell is pandas or numpy? . Let me explain... . &quot;Pandas&quot; is a software library writtten for the Python programming language for data manipulation and analysis. More specificly, it offers data structrues and operations for manipulating numerical tables and time series. . &quot;Numpy is a library for the Python programming language, adding support for large, multi-dimenstional arrays and matrices, along with a large collection of high-level mathematical functions to operate on these arrays. . Hooo, that&#39;s a lot to take in, but we may be ok for now. . After we import pandas and numpy, let&#39;s import the heart disease data from Github: . heart_disease = pd.read_csv(&quot;https://raw.githubusercontent.com/CongLiu-CN/zero-to-mastery-ml/master/data/heart-disease.csv&quot;) heart_disease.head() . age sex cp trestbps chol fbs restecg thalach exang oldpeak slope ca thal target . 0 63 | 1 | 3 | 145 | 233 | 1 | 0 | 150 | 0 | 2.3 | 0 | 0 | 1 | 1 | . 1 37 | 1 | 2 | 130 | 250 | 0 | 1 | 187 | 0 | 3.5 | 0 | 0 | 2 | 1 | . 2 41 | 0 | 1 | 130 | 204 | 0 | 0 | 172 | 0 | 1.4 | 2 | 0 | 2 | 1 | . 3 56 | 1 | 1 | 120 | 236 | 0 | 1 | 178 | 0 | 0.8 | 2 | 0 | 2 | 1 | . 4 57 | 0 | 0 | 120 | 354 | 0 | 1 | 163 | 1 | 0.6 | 2 | 0 | 2 | 1 | . Why do we need to split data into X &amp; y? . --Intentionally blank line-- . X = heart_disease.drop(&quot;target&quot;, axis=1) y = heart_disease[&quot;target&quot;] . 2. Choose the right model and hyperparameters . from sklearn.ensemble import RandomForestClassifier clf = RandomForestClassifier() . # We&#39;ll keep the deafault hyperparameters clf.get_params() . {&#39;bootstrap&#39;: True, &#39;ccp_alpha&#39;: 0.0, &#39;class_weight&#39;: None, &#39;criterion&#39;: &#39;gini&#39;, &#39;max_depth&#39;: None, &#39;max_features&#39;: &#39;auto&#39;, &#39;max_leaf_nodes&#39;: None, &#39;max_samples&#39;: None, &#39;min_impurity_decrease&#39;: 0.0, &#39;min_impurity_split&#39;: None, &#39;min_samples_leaf&#39;: 1, &#39;min_samples_split&#39;: 2, &#39;min_weight_fraction_leaf&#39;: 0.0, &#39;n_estimators&#39;: 100, &#39;n_jobs&#39;: None, &#39;oob_score&#39;: False, &#39;random_state&#39;: None, &#39;verbose&#39;: 0, &#39;warm_start&#39;: False} . 3. Fit the model to the training data . from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2) . clf.fit(X_train, y_train); . Make a prediction: . y_preds = clf.predict(X_test) y_preds . array([0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0], dtype=int64) . y_test . 256 0 24 1 266 0 294 0 131 1 .. 14 1 146 1 275 0 25 1 168 0 Name: target, Length: 61, dtype: int64 . 4. Evaluate the model on the training data and test data . clf.score(X_train, y_train) . 1.0 . clf.score(X_test, y_test) . 0.9016393442622951 . from sklearn.metrics import classification_report, confusion_matrix, accuracy_score . print(classification_report(y_test, y_preds)) . precision recall f1-score support 0 0.92 0.86 0.89 28 1 0.89 0.94 0.91 33 accuracy 0.90 61 macro avg 0.90 0.90 0.90 61 weighted avg 0.90 0.90 0.90 61 . confusion_matrix(y_test, y_preds) . array([[24, 4], [ 2, 31]], dtype=int64) . accuracy_score(y_test, y_preds) . 0.9016393442622951 . 5. Improve a model . Try different amount of n_estimators . np.random.seed(2020) . for i in range(10, 100, 10): print(f&quot;Trying model with {i} estimators...&quot;) clf = RandomForestClassifier(n_estimators=i).fit(X_train, y_train) print(f&quot;Model accuracy on test set: {clf.score(X_test, y_test)*100:.2f}%&quot;) print(&quot;&quot;) . Trying model with 10 estimators... Model accuracy on test set: 78.69% Trying model with 20 estimators... Model accuracy on test set: 90.16% Trying model with 30 estimators... Model accuracy on test set: 85.25% Trying model with 40 estimators... Model accuracy on test set: 88.52% Trying model with 50 estimators... Model accuracy on test set: 90.16% Trying model with 60 estimators... Model accuracy on test set: 90.16% Trying model with 70 estimators... Model accuracy on test set: 86.89% Trying model with 80 estimators... Model accuracy on test set: 85.25% Trying model with 90 estimators... Model accuracy on test set: 91.80% . 6. Save a model and load it . What is pickle? . https://www.pitt.edu/~naraehan/python3/pickling.html . import pickle . pickle.dump(clf, open(&quot;random_forest_model_1.pkl&quot;, &quot;wb&quot;)) . loaded_model = pickle.load(open(&quot;random_forest_model_1.pkl&quot;, &quot;rb&quot;)) loaded_model.score(X_test, y_test) # align with the last result . 0.9180327868852459 .",
            "url": "https://www.ai4love.xyz//2020/04/03/Introduction-to-Scikit-Learn.html",
            "relUrl": "/2020/04/03/Introduction-to-Scikit-Learn.html",
            "date": " • Apr 3, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This is where you put the contents of your About page. Like all your pages, it’s in Markdown format. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://www.ai4love.xyz//about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

}